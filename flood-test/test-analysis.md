# 压测分析报告（Phase 1–5）

## 0. 总体结论（先给结论）

1. **单纯“1000 并发连接”没有问题**：phase1 1000/1000 全部连上，连接与消息延迟都在毫秒级。
2. 真正出现明显退化的是 **phase4 / phase5**：两者的消息速率都达到 **4k msg/s 级别**，而且几乎全部来自 **`player_count_update`**。 
3. 从 Redis 指标看，**Redis 不是主要瓶颈**（平均延迟仍是 0.5–1.7ms 级别）；更像是 **Socket 广播风暴/发送队列积压 + Node event loop 拥塞** 在拖垮连接握手和消息处理。 

---

## 1. Phase 1 — Connection Capacity（连接容量基线）

**目标**：只测连接是否能稳定撑到 1000。

* 连接：1000/1000 成功，成功率 100%。
* 连接时间：P99 15.14ms。
* 消息延迟：P99 18.89ms。
* Event loop lag：P99 2.32ms。
* Redis：平均延迟 0.36ms。

**结论**：系统在“纯连接维度”可以稳定支撑 1000 并发。

---

## 2. Phase 2 — Low Frequency Messages（低频消息）

**目标**：1000 在线，低频提交/低频交互。

* 连接：1000/1000 成功。
* 事件统计：`submit_answer_success` 1510；同时出现 `submit_answer_error` 44,147。
* 消息延迟：P95/P99 ≈ 582,xxx ms（约 582 秒，接近整轮测试时长 602 秒）。 
* 但服务端指标并不“崩”：event loop lag 平均 1.06ms、P99 1.30ms；Redis 平均延迟 0.41ms。

**结论**：phase2 的主要现象是 **`submit_answer_error` 数量异常高**，导致延迟统计很可能被“未完成/失败请求的计时方式”污染；从 server/Redis 的平均指标看，不像是资源被打满造成的系统性延迟。
（你提到手动开题没问题，这点不冲突：即便开题正确，压测脚本仍可能触发大量“重复提交/不在允许窗口/不符合当前轮次状态”的错误；报告本身没有包含错误原因细分，所以只能确认“错误量非常大”。）

---

## 3. Phase 3 — Progressive Throughput（逐步提升吞吐）

**目标**：固定在线，逐步提升业务负载，观察系统进入拥塞的临界点。

* 连接：1000/1000 成功。
* 错误：`submit_answer_error` 386,218；成功 5,434。
* Event loop lag：平均 6.08ms，P99 48.84ms，最大 63.98ms；CPU 最大 100%。
* Redis：平均延迟 0.73ms（最大 68.81ms），ops/s 平均 699。

**结论**：phase3 已经出现明显的 **event loop 拥塞**（P99 到 48ms 级别），这会对 Socket 心跳/广播/握手产生影响；但同时 `submit_answer_error` 量依然极大，因此“消息延迟分钟级”这件事仍不能简单解读为纯系统延迟上升。

---

## 4. Phase 4 — Settlement Spike（结算尖峰，核心问题最明显）

这是你问的重点：phase4 的表现已经是“系统在压力下出现明显失败”。

### 4.1 现象（直接读数）

* 连接成功率掉到 **74.30%（743 成功 / 257 失败）**；连接 P99 **8.35s**。
* 消息速率 **4474 msg/s**，其中 `player_count_update` **352,799**，几乎占全部。
* 错误：`connect_error` 257；`submit_answer_error` 1000。
* 服务端 Event loop lag：平均 28.29ms，P99 42.32ms（持续拥塞态）；CPU 最大 94.2%。
* Redis：ops/s 平均 249，延迟平均 1.71ms（最大 42.14ms）。
* 结算尖峰窗口：2 秒内提交速率约 **499.54 提交/秒**。

### 4.2 解释：phase4 到底“卡在哪”

从指标组合看，phase4 更像是 **Socket 消息风暴（广播/发送队列）→ event loop backlog → 连接握手超时** 的链式问题，而不是 Redis 被打爆：

* 如果 Redis 是主瓶颈，通常会看到 ops/s、延迟出现持续高位；但你这里 Redis 平均延迟仍在毫秒级。
* 同期出现大量 `connect_error`（257）且连接 P99 达到 8 秒以上，典型就是 server 在高负载下无法及时处理新连接握手或升级。

---

## 5. Phase 5 — Reconnect Storm（重连风暴）

phase5 说明“重连动作本身”不一定会失败，但仍然暴露出同一个核心压力源：`player_count_update`。

### 5.1 现象

* 总尝试 1300，全成功；断开 300 后重连耗时 **562.95ms**，重连成功率 100%。 
* 消息速率 **4149 msg/s**，其中 `player_count_update` **344,814**。
* “消息延迟”统计非常大：P50 66,165ms，P99 70,626ms。
* 但服务端 event loop lag 平均只有 3.93ms（比 phase4 好很多），Redis 平均延迟 0.58ms。

### 5.2 解释：为什么“服务端看起来还行”，但“消息延迟很大”

这类组合（server lag 不高，但 client 侧统计延迟巨大）通常意味着：**消息在客户端/压测机侧出现排队或堆积**，或者“延迟统计口径”是按事件产生时间/发送时间算的，出现了明显 backlog。

而 backlog 的根源仍然指向：**`player_count_update` 的 fan-out 量非常大**。

---

## 6. `player_count_update` 是什么？为什么会有几十万条？

从事件名推断（并结合你项目语义），`player_count_update` 通常用于 **实时更新房间/游戏中的在线人数、玩家数量或幸存者人数**（例如 admin 大屏、show 屏、玩家端显示“当前人数”）。在 phase4/5，它几乎成了压测流量的绝对主导。 

关键点是：你报告里的“消息总数”通常是 **“所有客户端收到的消息总和”**。也就是说：

* 如果服务端对房间做一次广播，房间里有 700 人，那么在统计里可能记成 **700 条消息**（每个客户端各收到 1 条）。
* 因此 `player_count_update: 352,799` 不代表服务端真的发了 35 万次，而更可能是“广播次数 × 在线人数”的乘积。

用 phase4 粗略估算一下量级（只用于解释，不是精确还原）：

* phase4：`player_count_update` 352,799 条，持续 79.01 秒，约 4,474 条/秒（报告也给出了整体 4474 msg/s）。
* 当时活跃连接大约 696。
* 4,474 ÷ 696 ≈ **每秒约 6 次“广播级别”的 player_count_update**（每次广播会被 696 个客户端各记一次）。
  phase5 同理：约 4,149 msg/s，活跃 1000，则约 **每秒 4 次广播级别更新**。

所以，“数量巨大”本质上说明两件事之一（或两者叠加）：

1. `player_count_update` 在高负载阶段被**非常频繁地触发**（每秒多次）；
2. 该事件是**面向全房间广播**，导致在统计上出现指数级放大（fan-out）。

---

## 7. 五个 Phase 的一句话总结

* **Phase1**：1000 并发连接稳定通过，基础连接能力没问题。
* **Phase2**：低频交互下连接稳定，但 `submit_answer_error` 非常多，延迟统计被错误主导。
* **Phase3**：吞吐上升后出现 event loop 拥塞（P99 48ms、CPU 峰值 100%），同时仍有大量提交错误，延迟数据不可直接当“系统极限”。
* **Phase4**：最关键故障点——连接成功率掉到 74%，握手变慢（秒级），同时 `player_count_update` 导致 4k msg/s 级别风暴，event loop 持续拥塞。
* **Phase5**：重连动作本身能成功（300 人重连 ~563ms），但仍有 4k msg/s 的 `player_count_update` 风暴，客户端侧延迟统计出现严重积压。